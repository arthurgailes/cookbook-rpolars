## From a DuckDb file

### Use DuckDB engine on R object

Let's start by looking at how to use the DuckDb engine on R objects.  
There are 3 main possibilities:  

1. To use the DuckDB engine to query a R object with **dplyr**, you can use the `duckdb::duckdb_register()` method and then the `dplyr::tbl()` method to pass your dplyr instructions (**dplyr/DuckDB**). 

2. To use the DuckDB engine to query a R object with **the standard DBI methods**, you can use the `duckdb::duckdb_register()` method and then the `DBI::dbGetQuery()` method to pass your SQL query (**SQL/DuckDB**).

3. To use the DuckDB engine to query a R object in combination **with {arrow} package**, you can use the `arrow::to_duckdb()` and then pass your dplyr instructions (**dplyr/arrow/DuckDB**).

::: {.panel-tabset}

## dplyr/DuckDB

```{r}
#| label: duckdb-dplyr-benchmarking

duckdb_dplyr <- function(variables) {
  
  con <- DBI::dbConnect(duckdb::duckdb())

  duckdb::duckdb_register(con, "DataMultiTypes", DataMultiTypes)

  result <- tbl(con, "DataMultiTypes") |>
    
    mutate(
      # Conversion of 2 columns to Date format
      colDate1 = as.Date(colDate1),
      colDate2 = as.Date(colDate2)
    ) |>
    # Filter rows
    filter(
      colInt>2000 & colInt<8000
    ) |>
    # Grouping and aggregation
    group_by(colString) |> 
    summarise(
      min_colInt = min(colInt, na.rm = TRUE),
      mean_colInt = mean(colInt, na.rm = TRUE),
      mas_colInt = max(colInt, na.rm = TRUE),
      min_colNum = min(colNum, na.rm = TRUE),
      mean_colNum = mean(colNum, na.rm = TRUE),
      max_colNum = max(colNum, na.rm = TRUE)
    ) |>
    collect()
  
  DBI::dbDisconnect(con, shutdown=TRUE)

  return(result)
  
}
tic()
duckdb_dplyr()
toc()
```

## SQL/DuckDB

```{r}
#| label: duckdb-sql-benchmarking

duckdb_sql <- function(variables) {
  
  con <- DBI::dbConnect(duckdb::duckdb())

  duckdb::duckdb_register(con, "DataMultiTypes", DataMultiTypes)

  result <- DBI::dbGetQuery(
    con, 
    "SELECT colString,
           MIN(colInt) AS min_colInt,
           AVG(colInt) AS mean_colInt,
           MAX(colInt) AS max_colInt,
           MIN(colNum) AS min_colNum,
           AVG(colNum) AS mean_colNum,
           MAX(colNum) AS max_colNum
    FROM (
        SELECT colString,
               colInt,
               colNum
        FROM DataMultiTypes
        WHERE colInt > 2000 AND colInt < 8000
) AS filtered_data
GROUP BY colString;")
  
  DBI::dbDisconnect(con, shutdown=TRUE)
  
  return(result)
  
}
tic()
duckdb_sql()
toc()
```

## arrow/dplyr/DuckDB

```{r}
#| label: duckdb-dplyr-arrow-benchmarking

duckdb_arrow_dplyr <- function(variables) {
          
  result <- DataMultiTypes |>
    
    to_duckdb() |>
    
    mutate(
      # Conversion of 2 columns to Date format
      colDate1 = as.Date(colDate1),
      colDate2 = as.Date(colDate2)
    ) |>
    # Filter rows
    filter(
      colInt>2000 & colInt<8000
    ) |>
    # Grouping and aggregation
    group_by(colString) |> 
    summarise(
      min_colInt = min(colInt, na.rm = TRUE),
      mean_colInt = mean(colInt, na.rm = TRUE),
      mas_colInt = max(colInt, na.rm = TRUE),
      min_colNum = min(colNum, na.rm = TRUE),
      mean_colNum = mean(colNum, na.rm = TRUE),
      max_colNum = max(colNum, na.rm = TRUE)
    ) 
  
  return(result)
  
}
tic()
duckdb_arrow_dplyr()
toc()
```
:::

::: {.callout-tip}
One of the advantages of using the DuckDB engine and dplyr may be **to use a feature implemented by DuckDB but not yet by Arrow**. We can do the opposite, and return to the Arrow engine with `arrow::to_arrow()`.

However, the benchmark results are clear: SQL queries are by far the fastest! üèÜ
:::


### Results with DuckDB engine

```{r}
#| label: duckdb-engine-results-benchmarking
#| message: false
#| warning: false

microbenchmark(
  "dplyr/DuckDB" = duckdb_dplyr(),
  "SQL/DuckDB" = duckdb_sql(),
  "dplyr/arrow/DuckDB" = duckdb_arrow_dplyr(),
  times = 5
 )
```

### Queries on Duckdb files

Let's now look at how to perform queries on duckdb files.

For this comparison, we will use :

- For **SQL**, the `DBI::dbGetQuery()` method. In this way, we use the standard DBI methods to work from a DuckDb file.  

::: {.panel-tabset}

## SQL

```{r}
#| label: duckdb-dbfile-sql-benchmarking

duckdb_dbfile_sql <- function(variables) {
  
  con <- dbConnect(duckdb::duckdb(),
                 "Datasets/DataMultiTypes.duckdb")
  
  result <- dbGetQuery(
    con, 
    "SELECT colString,
           MIN(colInt) AS min_colInt,
           AVG(colInt) AS mean_colInt,
           MAX(colInt) AS max_colInt,
           MIN(colNum) AS min_colNum,
           AVG(colNum) AS mean_colNum,
           MAX(colNum) AS max_colNum
    FROM (
        SELECT colString,
               colInt,
               colNum
        FROM DataMultiTypes
        WHERE colInt > 2000 AND colInt < 8000
) AS filtered_data
GROUP BY colString;")
  
  dbDisconnect(con, shutdown=TRUE)
  
  return(result)
  
}
tic()
duckdb_sql()
toc()
```
:::

### Results for DuckDB file

```{r}
#| label: duckdb-results-benchmarking
#| message: false
#| warning: false

duckdb_bmk <- microbenchmark(
  "SQL from duckdb file" = duckdb_dbfile_sql(),
  times = 5
 )
duckdb_bmk
```
 
Note that the query with the standard DBI methods is faster than those with dplyr verbs üèÜ
