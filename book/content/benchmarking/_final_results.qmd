## Final results

### Performance

So what can we conclude?  
With which file format and which method is it fastest to execute the same request?  

Let's have a look! First, let's aggregate all the benchmark results:

```{r}
#| label: final-results-rbind

# Aggregation
bmk_results <- rbind(
  csv_bmk,
  unique_parquet_bmk,
  partitioned_parquet_bmk,
  duckdb_bmk
)
```

Let's do a quick sort on the `expr` column before plotting the results :

```{r}
#| label: data-manipulation-before-plotting
#| code-fold: true
#| message: false
#| warning: false
#| results: 'hide'

# Sort the results
bmk_results$expr <- reorder(bmk_results$expr, bmk_results$time, decreasing = TRUE)
```


```{r}
#| label: final-results-plot
#| echo: false
#| warning: false

speed_plot <- autoplot(bmk_results) +
  labs(title = "polars ‚öîÔ∏è others : Benchmark results!",
       subtitle = "üîé Right-click on the plot to open it wide in a new tab") +
  theme(plot.title = element_text(hjust = 0.5),
        legend.position = "none") +
  theme(axis.text.y = element_text(color = c("orange","orange","red","orange",
                                             "orange","orange","red","blue",
                                             "blue","blue","green","red","blue"))) +
  aes(color = expr) +
  scale_color_manual('Functions', values = c("orange","orange","red","orange",
                                             "orange","orange","red","blue",
                                             "blue","blue","green","red","blue"))
speed_plot
```

::: {.callout-important title="Final conclusions"}

üëâ **A few conclusions can be drawn from this section on benchmarking:**  

- It is **more efficient** to work **from a parquet or duckdb file**;  
- In terms of **execution speed**, there is **no great difference between a single parquet file and several partitioned parquet files** (although the gap will undoubtedly widen in favour of partitioned files if the size of the initial work file is increased);  
- **Lazy evaluation of polars performs best, followed by SQL queries executed directly on a duckdb file.** üèÜüèÜüèÜ   

:::

### Memory usage

We've just analysed the performance of the various alternatives to Polars, but what about R's memory usage?  

To do this, we're going to use `mem_change()` from `{pryr}` package. This method tells you how memory changes during code execution. Positive numbers represent an increase in the memory used by R, and negative numbers represent a decrease. 

```{r}
#| label: garbage-collection
#| echo: false
#| results: 'hide'
gc()
```

```{r}
#| label: memory-usage-results
#| echo: false

memory_usage_df <- data.frame(
  functions = c("polars (lazy) - from unique parquet file",
                "polars (lazy) - from partitioned parquet file",
                "SQL from duckdb file",
                "Duckdb and SQL - from unique parquet file",
                "arrow (eager) - from unique parquet file",
                "arrow (lazy) - from unique parquet file",
                "arrow (lazy) - from partitioned parquet file",
                "dplyr (Acero) - from csv file",
                "polars (eager) from csv file",
                "data.table - from csv file",
                "dplyr (duckdb) - from partitioned parquet file",
                "dplyr - from csv file",
                "R base - from csv file"),
  change = c(mem_change(parquet_polars_lazy()$collect()$to_data_frame()),
             mem_change(partitioned_parquet_polars_lazy()$to_data_frame()),
             mem_change(duckdb_dbfile_sql()),
             mem_change(parquet_duckdb_sql()),
             mem_change(arrow_eager()),
             mem_change(arrow_lazy() |> collect()),
             mem_change(partitioned_parquet_arrow_lazy()),
             mem_change(csv_arrow()),
             mem_change(csv_eager_polars()),
             mem_change(csv_dt()),
             mem_change(partitioned_parquet_dplyr_duckdb()),
             mem_change(csv_dplyr()),
             mem_change(csv_rbase()))
)

  
# Cr√©ation du graphique avec ggplot2
mem_plot <- ggplot(memory_usage_df, 
                   aes(x = change,
                       y = fct_relevel(
                         functions,
                          "R base - from csv file",
                         "dplyr - from csv file",                         
                         "dplyr (duckdb) - from partitioned parquet file",
                         "data.table - from csv file",                    
                         "polars (eager) from csv file" ,                 
                         "dplyr (Acero) - from csv file",                 
                         "arrow (lazy) - from partitioned parquet file", 
                         "arrow (lazy) - from unique parquet file",     
                         "arrow (eager) - from unique parquet file",      
                         "Duckdb and SQL - from unique parquet file",     
                         "SQL from duckdb file",                          
                         "polars (lazy) - from partitioned parquet file" ,
                         "polars (lazy) - from unique parquet file"))) +
  geom_col(fill = rev(c("orange","orange","red","orange",
                            "orange","orange","red","blue",
                            "blue","blue","green","red","blue"))) +
                              labs(title = "Memory consumption according to functions used",
                                   subtitle = "üîé Right-click on the plot to open it wide in a new tab",
                                   x = "Memory consumption", y = "Functions") +
    theme(axis.text.y = element_text(color = rev(c("orange","orange","red","orange",
                                             "orange","orange","red","blue",
                                             "blue","blue","green","red","blue"))))
mem_plot
```
